{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59b0590-f7dd-4563-a8d9-a31d6f4c15d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting is a method used in machine learning to reduce errors in predictive data analysis.\\nData scientists train machine learning software, called machine learning models, on labeled data to make\\nguesses about unlabeled data.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 1 :\n",
    "\"\"\"\n",
    "Boosting is a method used in machine learning to reduce errors in predictive data analysis.\n",
    "Data scientists train machine learning software, called machine learning models, on labeled data to make\n",
    "guesses about unlabeled data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984c9150-5a21-435b-8cc9-721023b4c650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting is a resilient method that curbs over-fitting easily.\\nOne disadvantage of boosting is that it is sensitive to outliers since every classifier is\\nobliged to fix the errors in the predecessors.\\nThus, the method is too dependent on outliers.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 2 :\n",
    "\"\"\"\n",
    "Boosting is a resilient method that curbs over-fitting easily.\n",
    "One disadvantage of boosting is that it is sensitive to outliers since every classifier is\n",
    "obliged to fix the errors in the predecessors.\n",
    "Thus, the method is too dependent on outliers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0b740f-089c-45a8-b039-f2f246a76a1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBoosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize\\ntraining errors.\\nIn boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is,\\neach model tries to compensate for the weaknesses of its predecessor.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 3 :\n",
    "\"\"\"\n",
    "Boosting is an ensemble learning method that combines a set of weak learners into a strong learner to minimize\n",
    "training errors.\n",
    "In boosting, a random sample of data is selected, fitted with a model and then trained sequentially—that is,\n",
    "each model tries to compensate for the weaknesses of its predecessor.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8371ae7d-8bae-477e-95d3-3502c4a52c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nThere are three types of Boosting Algorithms which are as follows: \\nAdaBoost (Adaptive Boosting) algorithm. Gradient Boosting algorithm. XG Boost algorithm.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 4 :\n",
    "\"\"\" \n",
    "There are three types of Boosting Algorithms which are as follows: \n",
    "AdaBoost (Adaptive Boosting) algorithm. Gradient Boosting algorithm. XG Boost algorithm.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6392aa3-1d69-4b1e-8286-04b406bea502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are many parameters, but below are a few key defaults.\\nlearning_rate=0.1 (shrinkage).\\nn_estimators=100 (number of trees).\\nmax_depth=3.\\nmin_samples_split=2.\\nmin_samples_leaf=1.\\nsubsample=1.0.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 5 :\n",
    "\"\"\"\n",
    "There are many parameters, but below are a few key defaults.\n",
    "learning_rate=0.1 (shrinkage).\n",
    "n_estimators=100 (number of trees).\n",
    "max_depth=3.\n",
    "min_samples_split=2.\n",
    "min_samples_leaf=1.\n",
    "subsample=1.0.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e929270-c714-4834-b1b4-27b34ae7fa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q No. 6 :\n",
    "# In boosting, a random sample of data is selected, fitted with a model and then trained sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d3b88b9-0420-4bc9-9630-643544035234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning.\\nIt is called Adaptive Boosting as the weights are re-assigned to each instance, \\nwith higher weights assigned to incorrectly classified instances.\\nIt works on the principle of learners growing sequentially. \\nExcept for the first, each subsequent learner is grown from previously grown learners.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 7 :\n",
    "\"\"\"\n",
    "AdaBoost algorithm, short for Adaptive Boosting, is a Boosting technique used as an Ensemble Method in Machine Learning.\n",
    "It is called Adaptive Boosting as the weights are re-assigned to each instance, \n",
    "with higher weights assigned to incorrectly classified instances.\n",
    "It works on the principle of learners growing sequentially. \n",
    "Except for the first, each subsequent learner is grown from previously grown learners.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6807db68-3e58-48f6-a094-ba1fe9421870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe exponential loss is used in the AdaBoost algorithm. \\nThe principal attraction of exponential loss in the context of additive modeling is computational.\\nThe additive expansion produced by AdaBoost is estimating onehalf of the log-odds of P(Y = 1|x).\\nThis justifies using its sign as the classification rule.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 8 :\n",
    "\"\"\"\n",
    "The exponential loss is used in the AdaBoost algorithm. \n",
    "The principal attraction of exponential loss in the context of additive modeling is computational.\n",
    "The additive expansion produced by AdaBoost is estimating onehalf of the log-odds of P(Y = 1|x).\n",
    "This justifies using its sign as the classification rule.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ed78111-2a0b-4a5a-97af-7368f79a527b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the boosting algorithm,AdaBoost ,those observations which were misclassified by the classifier \\nin the (m-1)th step have their weights increased in the mth step, \\nand those which were correctly classified have their weights decreased.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q No. 9 :\n",
    "\"\"\"\n",
    "In the boosting algorithm,AdaBoost ,those observations which were misclassified by the classifier \n",
    "in the (m-1)th step have their weights increased in the mth step, \n",
    "and those which were correctly classified have their weights decreased.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a8f07e-1fed-417b-b36e-b44fb16143f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe maximum number of estimators at which boosting is terminated. \\nIn case of perfect fit, the learning procedure is stopped early. Values must be in the range [1, inf)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Q no. 10 :\n",
    "\"\"\"\n",
    "The maximum number of estimators at which boosting is terminated. \n",
    "In case of perfect fit, the learning procedure is stopped early. Values must be in the range [1, inf)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
